{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import gensim \n",
    "import pandas as pd\n",
    "import multiprocessing \n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "STOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS))\n",
    "SYMBOLS = \" \".join(string.punctuation.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def argmax(iterable):\n",
    "    return max(enumerate(iterable), key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"config.json\") as json_file: \n",
    "    config = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_terms(item): \n",
    "    # Get term list and the original\n",
    "    terms = item[\"name\"]\n",
    "    \n",
    "    # Get the first 3\n",
    "    terms = terms.split(\" \")[:3]\n",
    "        \n",
    "    return {\n",
    "        \"name\": item[\"name\"], \n",
    "        \"terms\": [terms[2]], \n",
    "        \"keywords\": item[\"keywords\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_corpus(config): \n",
    " \n",
    "    search_terms = list(map(fetch_terms, config))\n",
    "    \n",
    "    result_file = open(\"sanity_check_filtering.csv\", \"w\")\n",
    "    \n",
    "    queries = [] \n",
    "    \n",
    "    for query in search_terms: \n",
    "        filename = \"CORPUS/%s.json\" % (query[\"name\"])\n",
    "        \n",
    "        with open(filename) as f: \n",
    "            corpus_list = json.load(f)\n",
    "\n",
    "        total_length = len(corpus_list)\n",
    "\n",
    "        keyword_relevant = 0\n",
    "\n",
    "        filtered_corpus_list = [] \n",
    "        \n",
    "        for corpus in corpus_list: \n",
    "            text = corpus[\"body\"]\n",
    "            tokens = text.split(\" \")\n",
    "                        \n",
    "            token_set = set([tok.lower() for tok in tokens])\n",
    "            keyword_set = set([tok.lower() for tok in query[\"keywords\"]])\n",
    "\n",
    "            if (token_set.intersection(keyword_set)): \n",
    "                keyword_relevant += 1\n",
    "                filtered_corpus_list.append(corpus)\n",
    "        \n",
    "        queries.append({\n",
    "            \"query\": query[\"name\"], \n",
    "            \"corpus\": filtered_corpus_list, \n",
    "        })\n",
    "        \n",
    "        print(\"%s,%d,%d,%f\" % (query[\"name\"], keyword_relevant, total_length, keyword_relevant/total_length), \n",
    "                              file=result_file)\n",
    "\n",
    "    result_file.close()\n",
    "    # Proceed pipeline with the filtered corpus list        \n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(article): \n",
    "    article = article.strip()\n",
    "    \n",
    "    # Remove newlines\n",
    "    article = article.replace(\"\\n\", \" \")\n",
    "    article = article.replace(\"\\r\", \" \")\n",
    "      \n",
    "    # Remove HTML symbols if any \n",
    "    article = article.replace(\"&amp;\", \"and\")\n",
    "    article = article.replace(\"&gt;\", \">\")\n",
    "    article = article.replace(\"&lt;\", \"<\")\n",
    "    \n",
    "    article = article.lower()\n",
    "    return article \n",
    "\n",
    "def tokenize(article): \n",
    "    tokens = article.split()\n",
    "            \n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetch_topics_for_query(name, article_list):    \n",
    "    documents = map(lambda doc: clean_text(doc[\"body\"]), article_list)\n",
    "    texts = [tokenize(document) for document in documents]\n",
    "    \n",
    "    dictionary = gensim.corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    ldaModel = gensim.models.ldamulticore.LdaMulticore(corpus=corpus, \n",
    "                                              num_topics=50,\n",
    "                                              id2word=dictionary,\n",
    "                                              passes=1, \n",
    "                                              workers=3)\n",
    "    \n",
    "    topics = ldaModel.show_topics(num_topics=10, log=True, formatted=True)\n",
    "\n",
    "    file = open(\"%s_topics.txt\" % name, 'w')\n",
    "    print(topics, file=file)\n",
    "    file.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_topics_for_queries(query_list):\n",
    "    df = pd.DataFrame(columns=[])\n",
    "    for query in query_list: \n",
    "        corpus = query[\"corpus\"]\n",
    "        fetch_topics_for_query(query[\"query\"], corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Binning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE = 1996 # Max range of 22 years\n",
    "RANGE = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bin_dates_for_query(name, article_list): \n",
    "\n",
    "    years = {}\n",
    "    \n",
    "    for x in range(0, RANGE): \n",
    "        years[x + BASE] = 0 \n",
    "    \n",
    "    for article in article_list: \n",
    "        year = article[\"date\"]\n",
    "        year = year[:4] # Hack - get the first 4 digits, year\n",
    "        year = int(year) \n",
    "        years[year] += 1\n",
    "        \n",
    "    \n",
    "    result = []\n",
    "    for x in range(BASE, BASE+RANGE): \n",
    "        result.append(years[x])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bin_dates(query_list):\n",
    "    cols = [\"Name\"] + [x + BASE for x in range(0, RANGE)]\n",
    "    df = pd.DataFrame()\n",
    "    for query in query_list: \n",
    "        name = query[\"query\"]\n",
    "        articles = query[\"corpus\"]\n",
    "        histogram = bin_dates_for_query(name, articles)\n",
    "        df = df.append([[name] + histogram])\n",
    "    df.columns = cols\n",
    "    df.to_csv(\"date_binning.csv\")\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment_of_query(args):\n",
    "    \n",
    "    name = args[0]\n",
    "    articles_list = args[1]\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer() \n",
    "    length = len(articles_list)\n",
    "    comp, pos, neu, neg = 0.0, 0.0, 0.0, 0.0\n",
    "    print(\"Processing\", name)\n",
    "    for article in articles_list: \n",
    "        text = article[\"body\"]\n",
    "        polarity = sid.polarity_scores(text) \n",
    "        comp += polarity['compound']\n",
    "        pos += polarity['pos']\n",
    "        neu += polarity['neu']\n",
    "        neg += polarity['neg']\n",
    "    \n",
    "    return [name, comp/length, pos/length, neu/length, neg/length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment(query_list):\n",
    "    cols = [\"Name\", \"Compound\", \"Pos\", \"Neu\", \"Neg\"]\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    CORES = 4\n",
    "    pool = multiprocessing.Pool(CORES)\n",
    "    args = [(query[\"query\"], query[\"corpus\"]) for query in query_list]\n",
    "    dfrows = pool.map(get_sentiment_of_query, args)\n",
    "    pool.close() \n",
    "    # Wait for map to finish\n",
    "    pool.join() \n",
    "    \n",
    "    \n",
    "    df = df.append(dfrows)\n",
    "    df.columns = cols\n",
    "    df.to_csv(\"category_sentiments.csv\")\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_list = filter_corpus(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fetch_topics_for_queries(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_binning_df = bin_dates(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SURVEILLANCE AND LEAKS\n",
      "Processing AUSTRIA AND PRIVACY AND EITHER HNA AGENCIES\n",
      "Processing BULGARIA AND PRIVACY AND EITHER AGENCIES\n",
      "Processing CYPRUS AND PRIVACY AND EITHER AGENCIES\n",
      "Processing AUSTRIA AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing BELGIUM AND PRIVACY AND EITHER AGENCIES\n",
      "Processing CYPRUS AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing CZECH REPUBLIC AND PRIVACY AND EITHER AGENCIES\n",
      "Processing BELGIUM AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing DENMARK AND PRIVACY AND EITHER AGENCIES\n",
      "Processing DENMARK AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing ESTONIA AND PRIVACY AND EITHER AGENCIES\n",
      "Processing CZECH REPUBLIC AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing BULGARIA AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing CROATIA AND PRIVACY AND EITHER AGENCIES\n",
      "Processing FINLAND AND PRIVACY AND EITHER AGENCIES\n",
      "Processing ESTONIA AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing FINLAND AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing FRANCE AND PRIVACY AND EITHER AGENCIES\n",
      "Processing CROATIA AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing GERMANY AND PRIVACY AND EITHER AGENCIES\n",
      "Processing HACKING AND SURVEILLANCE AND DISCLOSURE\n",
      "Processing FRANCE AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing HUNGARY AND PRIVACY AND EITHER AGENCIES\n",
      "Processing GERMANY AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing HUNGARY AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing GCHQ AND SURVEILLANCE\n",
      "Processing HUNGARY AND PRIVACY AND EITHER AGENCIES\n",
      "Processing IRELAND AND PRIVACY AND EITHER AGENCIES\n",
      "Processing IRELAND AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing ITALY AND PRIVACY AND EITHER AGENCIES\n",
      "Processing GREECE AND PRIVACY AND EITHER AGENCIES\n",
      "Processing ITALY AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing LATVIA AND PRIVACY AND EITHER AGENCIES\n",
      "Processing LATVIA AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing LITHUANIA AND PRIVACY AND EITHER AGENCIES\n",
      "Processing LITHUANIA AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing HUNGARY AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing LUXEMBOURG AND PRIVACY AND EITHER AGENCIES\n",
      "Processing LUXEMBOURG AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing GREECE AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing MALTA AND PRIVACY AND EITHER AGENCIES\n",
      "Processing NETHERLANDS AND PRIVACY AND EITHER AGENCIES\n",
      "Processing MALTA AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing PORTUGAL AND PRIVACY AND EITHER AGENCIES\n",
      "Processing PORTUGAL AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing ROMANIA AND PRIVACY AND EITHER AGENCIES\n",
      "Processing ROMANIA AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing SLOVAKIA AND PRIVACY AND EITHER AGENCIES\n",
      "Processing NETHERLANDS AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing POLAND AND PRIVACY AND EITHER AGENCIES\n",
      "Processing GCHQ AND PRIVACY\n",
      "Processing SLOVAKIA AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing SPAIN AND PRIVACY AND EITHER AGENCIES\n",
      "Processing SPAIN AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing POLAND AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing SWEDEN AND PRIVACY AND EITHER AGENCIES\n",
      "Processing SWEDEN AND SURVEILLANCE AND EITHER AGENCIES\n",
      "Processing UK AND PRIVACY AND EITHER AGENCIES\n",
      "Processing UK AND SURVEILLANCE AND EITHER AGENCIES\n"
     ]
    }
   ],
   "source": [
    "sentiment_df = get_sentiment(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
