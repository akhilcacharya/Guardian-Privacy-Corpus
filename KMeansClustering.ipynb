{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import scipy\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy\n",
    "from functools import reduce\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from string import punctuation\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import unicodedata\n",
    "stop = set(stopwords.words('english'))\n",
    "import os\n",
    "with open(\"config.json\") as json_file: \n",
    "    config = json.load(json_file)\n",
    "\n",
    "#break the descriptions into sentences and then break the sentences into tokens\n",
    "#remove punctuation and stop words\n",
    "#lowercase the tokens\n",
    "def tokenizer(text):\n",
    "    try:\n",
    "        tokens_ = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "        \n",
    "        tokens = []\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent\n",
    "\n",
    "        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n",
    "        tokens = list(filter(lambda t: t not in punctuation, tokens))\n",
    "        tokens = list(filter(lambda t: t not in [u\"'s\", u\"n't\", u\"...\", u\"''\", u'``', \n",
    "                                            u'\\u2014', u'\\u2026', u'\\u2013'], tokens))\n",
    "        filtered_tokens = []\n",
    "        for token in tokens:\n",
    "            if re.search('[a-zA-Z]', token):\n",
    "                filtered_tokens.append(token)\n",
    "\n",
    "        filtered_tokens = list(map(lambda token: token.lower(), filtered_tokens))\n",
    "\n",
    "        return filtered_tokens\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "#Set the paths\n",
    "path = os.path.abspath(os.path.dirname(\"./CORPUS/\"))\n",
    "outPath = os.path.abspath(os.path.dirname(\"./CLUSTER/\"))\n",
    "directory = path\n",
    "#Data for clustering\n",
    "datas = []\n",
    "#For each file \n",
    "for file in os.listdir(directory):\n",
    "    #concat all of the files together\n",
    "    data = pd.read_json(directory+\"/\"+file)\n",
    "    datas.append(data);\n",
    "\n",
    "    result = pd.concat(datas)\n",
    "    result.head\n",
    "    vectorizer = TfidfVectorizer(min_df=10, max_features=10000, tokenizer=tokenizer, ngram_range=(1, 2))\n",
    "    vz=vectorizer.fit_transform(list(data['title']))\n",
    "    num_clusters = 30\n",
    "    kmeans_model = MiniBatchKMeans(n_clusters=num_clusters, init='k-means++', n_init=1, \n",
    "                                   init_size=1000, batch_size=1000, verbose=False, max_iter=2000)\n",
    "\n",
    "    #Fit the models\n",
    "    kmeans = kmeans_model.fit(vz)\n",
    "    kmeans_clusters = kmeans.predict(vz)\n",
    "    kmeans_distances = kmeans.transform(vz)\n",
    "\n",
    "\n",
    "    outfile = open(outPath+'/'+os.path.splitext(file)[0]+'.txt', 'w')\n",
    "    for (i, data.count),category in zip(enumerate(data.title),data['title']):\n",
    "        s= \"Cluster \" + str(kmeans_clusters[i]) + \": \" + data.count + \"(distance: \" + str(kmeans_distances[i][kmeans_clusters[i]])+ \")\"\n",
    "        s=unicodedata.normalize('NFKD',s).encode('ascii','ignore')\n",
    "        print >>outfile,s\n",
    "        s='category: ',category;\n",
    "        s=unicode(s)\n",
    "        #Deal with unicode chars\n",
    "        s=unicodedata.normalize('NFKD',s).encode('ascii','ignore')\n",
    "        \n",
    "        print >>outfile,s\n",
    "        print >>outfile,'---'\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
